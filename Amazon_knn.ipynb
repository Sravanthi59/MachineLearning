{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib                         2.2.2    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip list installed | findstr matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in our data : (100000, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# using SQLite Table to read data.\n",
    "conn = sqlite3.connect('D:/datasets/Amazon_Fine_food_Reviews/amazon-fine-food-reviews/database.sqlite')\n",
    "\n",
    "filtered_data = pd.read_sql_query(\" SELECT * FROM Reviews WHERE Score != 3 LIMIT 100000\", conn)\n",
    "\n",
    "#print(filtered_data.columns)\n",
    "#print(filtered_data.head(2))\n",
    "# Give reviews with Score>3 a positive rating(1), and reviews with a score<3 a negative rating(0).\n",
    "def rating(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "#changing reviews with score less than 3 to be positive and vice-versa\n",
    "actualScore = filtered_data['Score']\n",
    "positiveNegative = actualScore.map(rating) \n",
    "filtered_data['Score'] = positiveNegative\n",
    "print(\"Number of data points in our data :\", filtered_data.shape)\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_unique_users = pd.read_sql_query(\"\"\"SELECT UserId, ProductId, ProfileName, Time, Score, Text, COUNT(*) FROM Reviews\n",
    "GROUP BY UserId\n",
    "HAVING COUNT(*)>1\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique users: (80668, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>Time</th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>COUNT(*)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#oc-R115TNMSPFT9I7</td>\n",
       "      <td>B007Y59HVM</td>\n",
       "      <td>Breyton</td>\n",
       "      <td>1331510400</td>\n",
       "      <td>2</td>\n",
       "      <td>Overall its just OK when considering the price...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#oc-R11D9D7SHXIJB9</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>Louis E. Emory \"hoppy\"</td>\n",
       "      <td>1342396800</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife has recurring extreme muscle spasms, u...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#oc-R11DNU2NBKQ23Z</td>\n",
       "      <td>B007Y59HVM</td>\n",
       "      <td>Kim Cieszykowski</td>\n",
       "      <td>1348531200</td>\n",
       "      <td>1</td>\n",
       "      <td>This coffee is horrible and unfortunately not ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#oc-R11O5J5ZVQE25C</td>\n",
       "      <td>B005HG9ET0</td>\n",
       "      <td>Penguin Chick</td>\n",
       "      <td>1346889600</td>\n",
       "      <td>5</td>\n",
       "      <td>This will be the bottle that you grab from the...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               UserId   ProductId             ProfileName        Time  Score  \\\n",
       "0  #oc-R115TNMSPFT9I7  B007Y59HVM                 Breyton  1331510400      2   \n",
       "1  #oc-R11D9D7SHXIJB9  B005HG9ET0  Louis E. Emory \"hoppy\"  1342396800      5   \n",
       "2  #oc-R11DNU2NBKQ23Z  B007Y59HVM        Kim Cieszykowski  1348531200      1   \n",
       "3  #oc-R11O5J5ZVQE25C  B005HG9ET0           Penguin Chick  1346889600      5   \n",
       "\n",
       "                                                Text  COUNT(*)  \n",
       "0  Overall its just OK when considering the price...         2  \n",
       "1  My wife has recurring extreme muscle spasms, u...         3  \n",
       "2  This coffee is horrible and unfortunately not ...         2  \n",
       "3  This will be the bottle that you grab from the...         3  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Unique users:',display_unique_users.shape)\n",
    "display_unique_users.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total No. of Reviews: 393063\n"
     ]
    }
   ],
   "source": [
    "print('Total No. of Reviews:',display_unique_users['COUNT(*)'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_duplicate_reviews= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE Score != 3 AND UserId=\"AR5J8UI46CURR\"\n",
    "ORDER BY ProductID\n",
    "\"\"\", conn)\n",
    "display_duplicate_reviews.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is observed (as shown in the table above) that the reviews data had many duplicate entries.\n",
    "Hence it was necessary to remove duplicates in order to get unbiased results for the analysis of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting data according to ProductId in ascending order\n",
    "sorted_data=filtered_data.sort_values('ProductId', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data after removing duplicated row entries: (87775, 10)\n"
     ]
    }
   ],
   "source": [
    "final_data=sorted_data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\n",
    "print('final_data after removing duplicated row entries:',final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64422</td>\n",
       "      <td>B000MIDROQ</td>\n",
       "      <td>A161DK06JJMCYF</td>\n",
       "      <td>J. E. Stephens \"Jeanne\"</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1224892800</td>\n",
       "      <td>Bought This for My Son at College</td>\n",
       "      <td>My son loves spaghetti so I didn't hesitate or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44737</td>\n",
       "      <td>B001EQ55RW</td>\n",
       "      <td>A2V0I904FH7ABY</td>\n",
       "      <td>Ram</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1212883200</td>\n",
       "      <td>Pure cocoa taste with crunchy almonds inside</td>\n",
       "      <td>It was almost a 'love at first bite' - the per...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId              ProfileName  \\\n",
       "0  64422  B000MIDROQ  A161DK06JJMCYF  J. E. Stephens \"Jeanne\"   \n",
       "1  44737  B001EQ55RW  A2V0I904FH7ABY                      Ram   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     3                       1      5  1224892800   \n",
       "1                     3                       2      4  1212883200   \n",
       "\n",
       "                                        Summary  \\\n",
       "0             Bought This for My Son at College   \n",
       "1  Pure cocoa taste with crunchy almonds inside   \n",
       "\n",
       "                                                Text  \n",
       "0  My son loves spaghetti so I didn't hesitate or...  \n",
       "1  It was almost a 'love at first bite' - the per...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_HelpfulnessNumerator_corrupted_reviews= pd.read_sql_query(\"\"\"\n",
    "SELECT *\n",
    "FROM Reviews\n",
    "WHERE HelpfulnessNumerator > HelpfulnessDenominator \n",
    "ORDER BY ProductID\n",
    "\"\"\", conn)\n",
    "display_HelpfulnessNumerator_corrupted_reviews.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HelpfulnessNumerator > HelpfulnessDenominator ,which is not practically possible hence these two rows too are removed from calcualtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data after removing corrupted HelpfulnessNumerator: (87773, 10)\n"
     ]
    }
   ],
   "source": [
    "final_data=final_data[final_data.HelpfulnessNumerator<=final_data.HelpfulnessDenominator]\n",
    "print('final_data after removing corrupted HelpfulnessNumerator:',final_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    73592\n",
       "0    14181\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing review text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Begin by removing the html tags\n",
    "\n",
    "2.Remove any punctuations or limited set of special characters like , or . or # etc.\n",
    "\n",
    "3.Check if the word is made up of english letters and is not alpha-numeric\n",
    "\n",
    "4.Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n",
    "\n",
    "5.Convert the word to lowercase\n",
    "\n",
    "6.Remove Stopwords\n",
    "\n",
    "7.Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dogs loves this chicken but its a product from China, so we wont be buying it anymore.  Its very hard to find any chicken products made in the USA but they are out there, but this one isnt.  Its too bad too because its a good product but I wont take any chances till they know what is going on with the china imports.\n",
      "__________________________________________________\n",
      "My Frenchbull is only given nylabone's to chew. He has had them since he was 7 weeks old. They are safe for him because he has a strong bite and they don't break off in large pieces that he could choke on. The Dinosaur Chew is perfect because it has so many places to hold and bite.  Dylabone is the only product I buy.\n",
      "__________________________________________________\n"
     ]
    }
   ],
   "source": [
    "text_0=final_data['Text'].values[0]\n",
    "print(text_0)\n",
    "print('_'*50)\n",
    "text_100=final_data['Text'].values[100]\n",
    "print(text_100)\n",
    "print('_'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dogs loves this chicken but its a product from China, so we wont be buying it anymore.  Its very hard to find any chicken products made in the USA but they are out there, but this one isnt.  Its too bad too because its a good product but I wont take any chances till they know what is going on with the china imports.\n",
      "__________________________________________________\n",
      "My Frenchbull is only given nylabone's to chew. He has had them since he was 7 weeks old. They are safe for him because he has a strong bite and they don't break off in large pieces that he could choke on. The Dinosaur Chew is perfect because it has so many places to hold and bite.  Dylabone is the only product I buy.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# remove urls from text python: https://stackoverflow.com/a/40823105/4084039\n",
    "text_0 = re.sub(r\"http\\S+\", \"\", text_0)\n",
    "text_100 = re.sub(r\"http\\S+\", \"\", text_100)\n",
    "\n",
    "print(text_0)\n",
    "print('_'*50)\n",
    "print(text_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My dogs loves this chicken but its a product from China, so we wont be buying it anymore.  Its very hard to find any chicken products made in the USA but they are out there, but this one isnt.  Its too bad too because its a good product but I wont take any chances till they know what is going on with the china imports.\n",
      "==================================================\n",
      "My Frenchbull is only given nylabone's to chew. He has had them since he was 7 weeks old. They are safe for him because he has a strong bite and they don't break off in large pieces that he could choke on. The Dinosaur Chew is perfect because it has so many places to hold and bite.  Dylabone is the only product I buy.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "# https://stackoverflow.com/questions/16206380/python-beautifulsoup-how-to-remove-all-tags-from-an-element\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(text_0, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)\n",
    "\n",
    "soup = BeautifulSoup(text_100, 'lxml')\n",
    "text = soup.get_text()\n",
    "print(text)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Frenchbull is only given nylabone is to chew. He has had them since he was 7 weeks old. They are safe for him because he has a strong bite and they do not break off in large pieces that he could choke on. The Dinosaur Chew is perfect because it has so many places to hold and bite.  Dylabone is the only product I buy.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "text_100 = decontracted(text_100)\n",
    "print(text_100)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Frenchbull is only given nylabone is to chew. He has had them since he was  weeks old. They are safe for him because he has a strong bite and they do not break off in large pieces that he could choke on. The Dinosaur Chew is perfect because it has so many places to hold and bite.  Dylabone is the only product I buy.\n"
     ]
    }
   ],
   "source": [
    "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
    "text_100 = re.sub(\"\\S*\\d\\S*\", \"\", text_100).strip()\n",
    "print(text_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Frenchbull is only given nylabone is to chew He has had them since he was weeks old They are safe for him because he has a strong bite and they do not break off in large pieces that he could choke on The Dinosaur Chew is perfect because it has so many places to hold and bite Dylabone is the only product I buy \n"
     ]
    }
   ],
   "source": [
    "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
    "text_100 = re.sub('[^A-Za-z0-9]+', ' ', text_100)\n",
    "print(text_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 87773/87773 [00:48<00:00, 1800.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Pre processing all reviews\n",
    "from tqdm import tqdm\n",
    "preprocessed_reviews = []\n",
    "# tqdm is for printing the status bar\n",
    "for Text in tqdm(final_data['Text'].values):\n",
    "    Text = re.sub(r\"http\\S+\", \"\", Text)\n",
    "    Text = BeautifulSoup(Text, 'lxml').get_text()\n",
    "    Text = decontracted(Text)\n",
    "    Text = re.sub(\"\\S*\\d\\S*\", \"\", Text).strip()\n",
    "    Text = re.sub('[^A-Za-z]+', ' ', Text)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    Text = ' '.join(e.lower() for e in Text.split() if e.lower() not in stopwords)\n",
    "    preprocessed_reviews.append(Text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 87773/87773 [00:37<00:00, 2358.54it/s]\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "# Pre processing all review summary\n",
    "from tqdm import tqdm\n",
    "preprocessed_summary = []\n",
    "# tqdm is for printing the status bar\n",
    "for Text in tqdm(final_data['Summary'].values):\n",
    "    Text = re.sub(r\"http\\S+\", \"\", Text)\n",
    "    Text = BeautifulSoup(Text, 'lxml').get_text()\n",
    "    Text = decontracted(Text)\n",
    "    Text = re.sub(\"\\S*\\d\\S*\", \"\", Text).strip()\n",
    "    Text = re.sub('[^A-Za-z]+', ' ', Text)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    Text = ' '.join(e.lower() for e in Text.split() if e.lower() not in stopwords)\n",
    "    preprocessed_summary.append(Text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 features: ['aa', 'aaa', 'aaaa', 'aaaaa', 'aaaaaaaaaaaa', 'aaaaaaaaaaaaaaa', 'aaaaaaahhhhhh', 'aaaaaaarrrrrggghhh', 'aaaaaawwwwwwwwww', 'aaaaah']\n",
      "(87773, 54904)\n",
      "unique words in all reviews: 54904\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "CountVect=CountVectorizer()\n",
    "CountVect.fit(preprocessed_reviews)\n",
    "print('first 10 features:',CountVect.get_feature_names()[0:10])\n",
    "\n",
    "trans_preprocessed_reviews=CountVect.transform(preprocessed_reviews)\n",
    "print(trans_preprocessed_reviews.shape)\n",
    "print('unique words in all reviews:',trans_preprocessed_reviews.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BI-Grams,n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the type of count vectorizer  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "the shape of out text BOW vectorizer  (87773, 5000)\n",
      "the number of unique words including both unigrams and bigrams  5000\n"
     ]
    }
   ],
   "source": [
    "#bi-gram, tri-gram and n-gram\n",
    "\n",
    "#removing stop words like \"not\" should be avoided before building n-grams\n",
    "# count_vect = CountVectorizer(ngram_range=(1,2))\n",
    "# please do read the CountVectorizer documentation http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# you can choose these numebrs min_df=10, max_features=5000, of your choice\n",
    "count_vect = CountVectorizer(ngram_range=(1,2), min_df=10, max_features=5000)\n",
    "final_bigram_counts = count_vect.fit_transform(preprocessed_reviews)\n",
    "\n",
    "print(\"the type of count vectorizer \",type(final_bigram_counts))\n",
    "print(\"the shape of out text BOW vectorizer \",final_bigram_counts.get_shape())\n",
    "print(\"the number of unique words including both unigrams and bigrams \", final_bigram_counts.get_shape()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency-Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 features: ['aa', 'aafco', 'aback', 'abandon', 'abandoned', 'abdominal', 'ability', 'able', 'able add', 'able brew']\n",
      "(87773, 51709)\n",
      "unique words in all reviews: 51709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf=TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "\n",
    "tf_idf.fit(preprocessed_reviews)\n",
    "print('first 10 features:',tf_idf.get_feature_names()[0:10])\n",
    "\n",
    "fianl_tfidf=tf_idf.transform(preprocessed_reviews)\n",
    "print(fianl_tfidf.shape)\n",
    "print('unique words in all reviews:',fianl_tfidf.shape[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['dogs', 'loves', 'chicken', 'product', 'china', 'wont', 'buying', 'anymore', 'hard', 'find', 'chicken', 'products', 'made', 'usa', 'one', 'isnt', 'bad', 'good', 'product', 'wont', 'take', 'chances', 'till', 'know', 'going', 'china', 'imports'], ['dogs', 'love', 'saw', 'pet', 'store', 'tag', 'attached', 'regarding', 'made', 'china', 'satisfied', 'safe'], ['infestation', 'fruitflies', 'literally', 'everywhere', 'flying', 'around', 'kitchen', 'bought', 'product', 'hoping', 'least', 'get', 'rid', 'weeks', 'fly', 'stuck', 'going', 'around', 'notepad', 'squishing', 'buggers', 'success', 'rate', 'day', 'clearly', 'product', 'useless', 'even', 'dabbed', 'red', 'wine', 'banana', 'top', 'column', 'week', 'really', 'attracted', 'red', 'wine', 'glass', 'still', 'nothing', 'get', 'stuck', 'actually', 'saw', 'second', 'fly', 'land', 'watched', 'flapped', 'wings', 'frantically', 'within', 'secs', 'unstuck', 'product', 'total', 'waste', 'money'], ['worst', 'product', 'gotten', 'long', 'time', 'would', 'rate', 'no', 'star', 'could', 'simply', 'not', 'catch', 'single', 'fly', 'bug', 'sort', 'went', 'hardware', 'store', 'bought', 'old', 'fashioned', 'spiral', 'fly', 'paper', 'effective', 'unuasual', 'influx', 'flys', 'house', 'fall', 'needed', 'something'], ['wish', 'would', 'read', 'reviews', 'making', 'purchase', 'basically', 'cardsotck', 'box', 'sticky', 'outside', 'pink', 'ish', 'things', 'look', 'like', 'entrances', 'trap', 'pictures', 'no', 'inside', 'trap', 'flies', 'stuck', 'outside', 'basically', 'fly', 'paper', 'horribly', 'horribly', 'horribly', 'overpriced', 'favor', 'get', 'fly', 'paper', 'fly', 'strips', 'yuck', 'factor', 'much', 'cheaper'], ['happy', 'item', 'many', 'flies', 'disturbing', 'kitchen', 'put', 'product', 'near', 'window', 'works', 'fantastically'], ['thing', 'item', 'trapped', 'fruit', 'flies', 'not', 'work', 'fly', 'trap', 'would', 'not', 'recommend'], ['nurturing', 'plant', 'work', 'well', 'decided', 'repot', 'larger', 'pot', 'must', 'gotten', 'funky', 'soil', 'soon', 'repotted', 'overrun', 'large', 'colony', 'small', 'flies', 'swatting', 'getting', 'bug', 'guts', 'hand', 'frequently', 'stopped', 'bothering', 'gross', 'exclamations', 'fact', 'city', 'san', 'francisco', 'compost', 'aggravated', 'situation', 'bought', 'one', 'nitfy', 'traps', 'set', 'plant', 'voila', 'instead', 'hands', 'getting', 'daily', 'dose', 'bug', 'guts', 'trap', 'lined', 'tiny', 'suckers', 'months', 'trap', 'pretty', 'much', 'covered', 'gross', 'part', 'need', 'order', 'another', 'one', 'not', 'sure', 'well', 'work', 'larger', 'inserts', 'works', 'really', 'well', 'tiny', 'annoying', 'flies'], ['placed', 'around', 'house', 'several', 'days', 'setup', 'fly', 'attracting', 'trap', 'vicinity', 'literally', 'watched', 'flies', 'avoid', 'trap', 'days', 'excited', 'see', 'one', 'lil', 'bugger', 'land', 'surface', 'disappointed', 'approached', 'examine', 'flew', 'away', 'paper', 'tacky', 'pressure', 'human', 'finger', 'applied', 'small', 'flies', 'hardly', 'trap'], ['please', 'not', 'waste', 'money', 'fly', 'trap', 'absolutely', 'useless', 'bought', 'two', 'product', 'kitchen', 'always', 'bbq', 'go', 'pool', 'fly', 'get', 'house', 'started', 'make', 'jokes', 'product', 'not', 'one', 'fly', 'far', 'gone', 'since', 'hang', 'weeks', 'ago', 'terrible', 'product']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "# Train your own Word2Vec model using your own text corpus\n",
    "i=0\n",
    "list_of_sents=[]\n",
    "for review in preprocessed_reviews:\n",
    "    list_of_sents.append(review.split())\n",
    "print(list_of_sents[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fantastic', 0.8487455248832703), ('excellent', 0.8304535150527954), ('awesome', 0.8162674903869629), ('terrific', 0.7988094091415405), ('wonderful', 0.7926963567733765), ('good', 0.7916631698608398), ('perfect', 0.7444168925285339), ('amazing', 0.731562614440918), ('fabulous', 0.6899922490119934), ('nice', 0.6759129762649536)]\n",
      "==================================================\n",
      "[('greatest', 0.7658005952835083), ('tastiest', 0.7239884734153748), ('best', 0.7095773220062256), ('nastiest', 0.7017833590507507), ('nicest', 0.6796818971633911), ('coolest', 0.6442915201187134), ('disgusting', 0.6400687098503113), ('surpass', 0.638436496257782), ('wins', 0.6310152411460876), ('hottest', 0.6301690936088562)]\n"
     ]
    }
   ],
   "source": [
    "w2v_model=Word2Vec(list_of_sents,min_count=5,size=50, workers=4)\n",
    "print(w2v_model.wv.most_similar('great'))\n",
    "print('='*50)\n",
    "print(w2v_model.wv.most_similar('worst'))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words that occured minimum 5 times  17386\n",
      "sample words  ['dogs', 'loves', 'chicken', 'product', 'china', 'wont', 'buying', 'anymore', 'hard', 'find', 'products', 'made', 'usa', 'one', 'isnt', 'bad', 'good', 'take', 'chances', 'till', 'know', 'going', 'imports', 'love', 'saw', 'pet', 'store', 'tag', 'attached', 'regarding', 'satisfied', 'safe', 'infestation', 'literally', 'everywhere', 'flying', 'around', 'kitchen', 'bought', 'hoping', 'least', 'get', 'rid', 'weeks', 'fly', 'stuck', 'squishing', 'buggers', 'success', 'rate']\n"
     ]
    }
   ],
   "source": [
    "w2v_words = list(w2v_model.wv.vocab)\n",
    "print(\"number of words that occured minimum 5 times \",len(w2v_words))\n",
    "print(\"sample words \", w2v_words[0:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVG Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 87773/87773 [05:12<00:00, 280.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87773\n",
      "50\n",
      "[ 0.12575598  0.46159474 -0.09309819  1.37069529 -0.49098364 -0.87857422\n",
      " -0.46466648  0.30025681  0.26689559 -0.43047276  0.21443228  0.44285296\n",
      "  0.45005171 -0.06647766  0.81688548 -0.31373119  0.46187448  0.09634926\n",
      " -0.83351827 -0.20564972  0.24990304 -0.77213698  0.11448248  0.30530376\n",
      " -0.59726662 -0.28261033  0.03641088  0.85926547 -0.73224569  0.24772421\n",
      "  0.1414935   0.27890404  0.0173071   0.09323656  0.26079548 -0.33653816\n",
      "  0.67106487 -0.80707854 -0.01311716  0.48671423  0.14546982  0.57295235\n",
      "  0.66897004 -0.01995566  0.15799741  0.12079119 -0.69715988 -0.243065\n",
      " -0.48326728 -0.54766804]\n"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sents): # for each review/sentence\n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "    if cnt_words != 0:\n",
    "        sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))\n",
    "print(sent_vectors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf weighted word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\n",
    "model = TfidfVectorizer()\n",
    "tf_idf_matrix = model.fit_transform(preprocessed_reviews)\n",
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 87773/87773 [57:09<00:00, 25.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF weighted Word2Vec\n",
    "tfidf_feat = model.get_feature_names() # tfidf words/col-names\n",
    "# final_tf_idf is the sparse matrix with row= sentence, col=word and cell_val = tfidf\n",
    "\n",
    "tfidf_sent_vectors = []; # the tfidf-w2v for each sentence/review is stored in this list\n",
    "for sent in tqdm(list_of_sents): # for each review/sentence \n",
    "    sent_vec = np.zeros(50) # as word vectors are of zero length\n",
    "    weight_sum =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        if word in w2v_words and word in tfidf_feat:\n",
    "            vhttp://localhost:8888/notebooks/Amazon_knn.ipynb#K-NN-Brute-Force-on-BOWec = w2v_model.wv[word]\n",
    "            # dictionary[word] = idf value of word in whole courpus\n",
    "            # sent.count(word) = tf valeus of word in this review\n",
    "            tf_idf = dictionary[word]*(sent.count(word)/len(sent))\n",
    "            sent_vec += (vec * tf_idf)\n",
    "            weight_sum += tf_idf\n",
    "    if weight_sum != 0:\n",
    "        sent_vec /= weight_sum\n",
    "    tfidf_sent_vectors.append(sent_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-NN Brute Force on BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'bincount'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-ae03b0136531>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneighbors\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python1\\lib\\site-packages\\sklearn\\neighbors\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkd_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdist_metrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDistanceMetric\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkneighbors_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mradius_neighbors_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0munsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclassification\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRadiusNeighborsClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python1\\lib\\site-packages\\sklearn\\neighbors\\graph.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# License: BSD 3 clause (C) INRIA, University of Amsterdam\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKNeighborsMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRadiusNeighborsMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0munsupervised\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mNearestNeighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python1\\lib\\site-packages\\sklearn\\neighbors\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mkd_tree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKDTree\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPAIRWISE_DISTANCE_FUNCTIONS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_get_n_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python1\\lib\\site-packages\\sklearn\\metrics\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mranking\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcoverage_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Python1\\lib\\site-packages\\sklearn\\metrics\\ranking.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextmath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstable_cumsum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbincount\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marray_equal\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrankdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'bincount'"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "X=preprocessed_reviews\n",
    "y=np.array(final_data['Score'])\n",
    "\n",
    "X_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.3)\n",
    "\n",
    "text_model=CountVectorizer()\n",
    "\n",
    "final_X_train=text_model.fit_transform(X_train)\n",
    "final_Xcv=text_model.transform(X_cv)\n",
    "final_X_test=text_model.transform(X_test)\n",
    "\n",
    "auc_cv=[]\n",
    "auc_train=[]\n",
    "\n",
    "K=list(range(1,30,4))\n",
    "cv_scores=[]\n",
    "\n",
    "for i in K:\n",
    "    knn=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute',leaf_size=30, p=2, metric='cosine')\n",
    "    knn.fit(final_X_train, y_train)\n",
    "    pred = knn.predict_proba(final_Xcv)[:,1]\n",
    "    auc_cv.append(roc_auc_score(y_cv,pred))\n",
    "    pred1=knn.predict_proba(final_Xtr)[:,1]\n",
    "    auc_train.append(roc_auc_score(y_tr,pred1))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn=KNeighborsClassifier(n_neighbors=29,weights='uniform',algorithm='brute',leaf_size=30, p=2, metric='cosine')\n",
    "knn.fit(final_X_train,y_tr)\n",
    "predi=knn.predict_proba(final_Xtest)[:,1]\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, predi)\n",
    "pred=knn.predict_proba(final_X_train)[:,1]\n",
    "fpr2,tpr2,thresholds2=metrics.roc_curve(y_tr,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K-NN Brute Force on tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "X=preprocessed_reviews\n",
    "y=np.array(final['Score'])\n",
    "\n",
    "tf_idf_vect = TfidfVectorizer(ngram_range=(1,2), min_df=10)\n",
    "\n",
    "X_1, X_test, y_1, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_1, y_1, test_size=0.3)\n",
    "\n",
    "final_Xtrain=tf_idf_vect.fit_transform(X_train)\n",
    "final_Xcv=tf_idf_vect.transform(X_cv)\n",
    "final_Xtest=tf_idf_vect.transform(X_test)\n",
    "\n",
    "auc_cv=[]\n",
    "auc_train=[]\n",
    "K=[]\n",
    "\n",
    "for i in range(1,50,4):\n",
    "    knn=KNeighborsClassifier(n_neighbors=i,weights='uniform',algorithm='brute',leaf_size=30, p=2, metric='cosine')\n",
    "    knn.fit(final_Xtr, y_tr)\n",
    "    pred = knn.predict_proba(final_Xcv)[:,1]\n",
    "    pred1=knn.predict_proba(final_Xtr)[:,1]\n",
    "    auc_cv.append(roc_auc_score(y_cv,pred))\n",
    "    auc_train.append(roc_auc_score(y_tr,pred1))\n",
    "    K.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
